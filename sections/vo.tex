\chapter{Classical Visual Odometry}
\label{ch:vo}
\epigraph{Eventually, my eyes were opened, and I really understood nature.}{\textsc{Claude Monet}}

\begin{figure}[h!]
\begin{center}
		\includegraphics[width=0.95\textwidth]{classical-vo/vo_taxonomy.pdf}
		\caption{A taxonomy of different types of visual odometry.}
  	\label{fig:vo_taxonomy}
\end{center}
\end{figure}


Visual odometry (VO) has a rich history in mobile robotics and computer vision. As this dissertation largely deals with the improvement of a baseline visual odometry pipeline, we first outline the components of what we have chosen to be a canonical VO system. For a seminal tutorial on visual odometry and its more general cousin, visual SLAM, we refer the reader to two seminal papers: \cite{Scaramuzza2011-qr} and \cite{Cadena2016-ds}.

\section{A taxonomy of VO}

VO can be largely divided along two dimensions (c.f. \Cref{fig:vo_taxonomy}): the type of camera (monocular vs. stereo) and the type of data association (indirect, or feature-based vs. direct, or pixel intensity-based). 

\textbf{Monocular vs. Stereo:}
The first distinction is based on the type of camera used by the VO pipeline. Monocular VO methods use a single camera to infer motion and can use a single compact, low-power vision sensor. They do not require any extrinsic calibration but must rely on known visual cues or external information (e.g., wheel odometry, inertial measurements) to provide metric egomotion estimates. Conversely, stereo VO methods use a stereo camera to triangulate objects with metric scale. This allows stereo VO to provide metricaly-acurate egomotion estimates. However, stereo methods rely on accurate extrinsic calibration, and their ability to resolve depth is limited by the baseline distance between the stereo pair. 

\textbf{Direct vs. Indirect:}
The second distinction is based on the type of data association used to match sequential images. Direct methods make the assumption of brightness constancy, and attempt to \textit{directly} maximize the similarity of pixel intensities. Indirect methods, however, rely on image features detectors to extract a set of salient landmarks, and then match these landmarks across images (typically through some sort of invariant descriptor).

\section{A classical VO pipeline}

In this thesis, we apply our learned pseudo-sensors to a baseline stereo, indirect visual odometry pipeline largely based on the work of \cite{furgale_phd11}. We choose this baseline system for its computational efficiency and robustness. We briefly summarize the main components of the dpipeline here.

\begin{figure}[h!]
\begin{center}
		\includegraphics[width=0.98\textwidth]{classical-vo/stereo_vo_pipeline.pdf}
		\caption{A `classical' stereo visual odometry pipeline consists of several distinct components that have interpretable inputs and outputs.}
  	\label{fig:vo_stereo_vo_pipeline}
\end{center}
\end{figure}

\subsection{Preprocessing}


\begin{figure}[h!]
     \centering
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
     		\includegraphics[width=0.98\textwidth]{classical-vo/stereo_camera}
			\caption{Ideal stereo camera.}
			 \label{fig:vo_stereo_camera}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.48\textwidth}
         \centering
         \includegraphics[width=0.75\textwidth]{classical-vo/stereo_rectification}
        \caption{Rectification and undistortion process. Figure adapted from \cite{florez2010}.}
         \label{fig:vo_undistort_recitfy}
	 \end{subfigure}
    \caption{Preprocessing components.}
        \label{fig:vo_preprocessing}
\end{figure}

During preprocessing, we use a lens model (assumed to be known apriori) to undistort each stereo image. Further, using the camera extrinsic parameters (also assumed to be known), we \textit{rectify} the stereo pair such that the images can be assumed to come from two cameras whose principal axes are parallel (\Cref{fig:vo_preprocessing}). Finally, we also assume that the stereo camera intrinsics are known a priori or compute them through a calibration process \citep{Furgale2013-sl}.

\subsection{Data association}

\subsubsection{Feature extraction and matching}

\begin{figure}[h!]
\begin{center}
		\includegraphics[width=0.88\textwidth]{classical-vo/libviso2_scan}
		\caption{Feature tracking using \texttt{libviso2}, taken from \cite{Geiger2011-xe}. Colours correspond to depth.}
  	\label{fig:vo_feature_tracking}
\end{center}
\end{figure}

In this thesis, we focus on indirect stereo visual odometry for its computational efficiency. Although a number of different types of indirect feature extraction and matching methods can be used towards this end, we choose to use the \texttt{viso2} \citep{geiger2011stereoscan} image feature extraction and matching algorithm. In \texttt{viso2}, features are extracted using blob and corner masks with non-minimum and non-maximum suppression. Unlike other features detectors that do not assume a particular camera motion, \texttt{viso2} assumes a smooth camera trajectory that permits fast matching through a simple sum-of-absolute-difference error metric of $11 \times 11$ windows of Sobel filter responses. Finally, features are matched across a stereo-pair and forward in time, to ensure that a single feature exists across two consecutive stereo camera poses. 

Each extract feature corresponds to a
point in space, expressed in homogeneous coordinates in the camera frame as
$\HomogeneousPoint{i}{t} := \Transpose{\bbm p_1 & p_2 & p_3 & p_4 \ebm} \in
\HomogeneousNumbers[3]$.  Given our intrinsics and extrinsic calibration parameters, our idealized stereo-camera model, $\ProjectionFunction$,
projects a landmark expressed in homogeneous coordinates into image space, so
that $\ImageLandmark{i}{t}$, the stereo pixel coordinates of landmark $i$ in the first camera pose at time $t$, is given
by 
\begin{equation}
	\ImageLandmark{i}{t} = \bbm u_l \\ v_l \\ u_r \\ v_r \ebm 
  = \ProjectionFunction(\HomogeneousPoint{i}{t}) 
  = \Matrix{M} \frac{1}{p_3}\HomogeneousPoint{i}{t},
\end{equation}
where
\begin{equation}
 \Matrix{M} = \bbm f & 0 & c_u & f \frac{b}{2} \\ 0 & f & c_v & 0 \\ f 
                        & 0 & c_u & -f\frac{b}{2} \\ 0 & f & c_v & 0 \ebm.
\end{equation}
Here, $\{c_u, c_v\}$, $\{f_u, f_v\}$, and $b$ are the principal points, focal
lengths and baseline of the stereo camera respectively. Note that in this
formulation, the stereo camera frame is centered between the two individual
lenses.  


\subsubsection{Outlier rejection}

To filter out any residual outlier matches, we use a three-point random sample consensus algorithm (RANSAC, \cite{FischlerRANSAC:1981}) based on an analytic solution to the six degree-of-freedom motion \citep{Umeyama1991-ws}.
  
\subsection{Motion solution}

To compute
$\Transform\in\text{SE}(3)$, the rigid transform between two subsequent stereo camera poses, we assume we have a set
of $N_t$ visual landmarks in each stereo pair at a time instance, $t$.

We triangulate landmarks in the first camera frame, $\ImageLandmark{i}{t}$, and re-project
them into the second frame, $\ImageLandmark{i}{t}'$. We model errors due to sensor noise
and quantization as a Gaussian distribution in image space with a covariance
$\ImageCovariance_{i,t}$ that may change for each feature or may be constant, $\ImageCovariance_{i,t} = \ImageCovariance$,
\begin{equation}
  p(\ImageLandmark{i}{t}' \vert \ImageLandmark{i}{t}, \Transform,
  \ImageCovariance_{i,t})
  =\NormalDistribution\left(\Vector{e}_{i,t}(\Transform); \Vector 0, \ImageCovariance_{i,t}\right), 
\end{equation}
where
\begin{equation}
 \Vector{e}_{i,t} = \ImageLandmark{i}{t}' - \ProjectionFunction( \Transform 
    \ProjectionFunction^{-1}( \ImageLandmark{i}{t} ) ).	
   \label{eq:image_error}
\end{equation}
  The maximum likelihood transform,
$\Transform^*$, is then given by 
\begin{equation}
  \Transform^* = \ArgMin{\Transform\in\text{SE}(3)}\sum_{i=1}^{N_t} 
  \Transpose{\Vector{e}_{i,t}} \ImageCovariance_{i,t}^{-1} \Vector{e}_{i,t}.
\end{equation}
This is a nonlinear least squares problem, and can be solved iteratively using
standard techniques. During iteration $n$, we represent the transform as the
product of an estimate $\Transform^{(n)}\in\text{SE}(3)$ and a perturbation
$\delta\Vector{\xi}\in\RealNumbers[6]$ represented in exponential
coordinates:
\begin{equation}
  \Transform = \exp{\left( \delta\Vector{\xi}^{\wedge}
  \right)} \Mean{\Transform}.
\end{equation}
Linearizing the transform for small perturbations $\delta\Vector{\xi}$
yields a linear least-squares problem:
\begin{equation}
\label{eq:vo_linleastsquares}
  \mathcal{L}(\delta \Vector{\xi}) = \frac{1}{2}\sum_{i=1}^{N_t} 
  \Transpose{\left(\Vector{e}_{i,t}^{(n)}
  - \Matrix J_{i,t}^{(n)} \delta\Vector{\xi}\right)}
\ImageCovariance_{i,t}^{-1}
 \left(\Vector{e}_{i,t}^{(n)}
 - \Matrix J_{i,t}^{(n)} \delta\Vector{\xi}\right)
  \end{equation}
Here, $\Matrix J_{i,t}^{(n)}$ is the Jacobian matrix of the reprojection error. \todo{add this to the appendix}

Rearranging, we see the minimizing perturbation is the solution to a
linear system of equations:
\begin{equation}
  \delta\Vector{\xi}^{(n)} = 
  \left( \sum_{i=1}^{N_t} \Transpose{\Matrix J}_{i,t}
  \ImageLandmarkCovariance{}{}^{-1} \Matrix J_{i,t} \right)^{-1}
  \sum_{i=1}^{N_t} \Transpose{\Matrix J}_{i,t}
  \ImageCovariance_{i,t}^{-1} \Vector{e}_{i,t}^{(n)}. 
\label{eq:least-squares-iteration}
\end{equation}
We then update the estimated transform and proceed to the next iteration,
\begin{equation}
  \Transform^{(n+1)} = \MatExp{\delta\Vector{\xi}^{(n)}} \Transform^{(n)}. \label{eq:update}
\end{equation}
There are many reasonable choices for both the initial transform
$\Transform^{(0)}$ and for the conditions under which we terminate
iteration. We initialize the estimated transform to identity, and iteratively
perform the update given by \cref{eq:update} until we see a relative change in
the squared error of less than one percent after an update. 

%\newpage
%\section{Pose Graph Optimization}
%
%\begin{wrapfigure}{r}{0.4\textwidth}
%  \vspace{-20pt}
%  \begin{center}
%	\includegraphics[width=0.38\textwidth]{classical-vo/pose_graph.pdf}
%  \end{center}
%    \vspace{-20pt}
%	\label{fig:math_pose_graph}
%	\caption{The formulation of pose graph relaxation can incorporate different probabilistic \textit{factors} that constrain each camera pose.}
%\end{wrapfigure} 
%
%
%we fused our probabilistic rotation regression with classical stereo visual odometry using pose graph relaxation implemented with the help of a Python-based factor graph library which we will publicize after the review process. Using that framework, we solved
%\begin{align}
%	\Transform_{1,w}^*, \Transform_{2,w}^* &= \ArgMin{\Transform_{1,w}, \Transform_{2,w}\in\text{SE}(3)}\mathcal{L}(\Estimate{\Transform}_{2,1}, \Estimate{\Rotation}_{2,1}) \\ & = \ArgMin{\Transform_{1,w}, \Transform_{2,w}\in\text{SE}(3)} \Vector{\xi}_\text{1,2}^T \Matrix{\Sigma}^{-1}_\text{vo} \Vector{\xi}_\text{1,2} + \Vector{\phi}_\text{1,2}^T \Matrix{\Sigma}^{-1}_{\text{hn}} \Vector{\phi}_\text{1,2} 
%\end{align}
%where
%\begin{equation}
%	\Vector{\xi}_\text{1,2} =  \MatLog{\left(\Transform_{2,w} \Transform_{1,w}^{-1} \right)\Estimate{\Transform}_{2,1}^{-1}},
%\end{equation}
%\begin{equation}
%	\Vector{\phi}_\text{1,2} =  \MatLog{\left(\Rotation_{2,w} \Rotation_{1,w}^{T} \right)\Estimate{\Rotation}_{2,1}^{T}},
%\end{equation}
%and $\Estimate{\Transform}_{2,1}$, $\Matrix{\Sigma}_\text{vo}$ and $\Estimate{\Rotation}_{2,1}$, $\Matrix{\Sigma}_{\text{hn}}$ were provided by our classical estimator and the HydraNet network respectively. Note that $\Matrix{\Sigma}_{\text{hn}} \in \Real^{3 \times 3} \geq 0$ while $\Matrix{\Sigma}_{\text{vo}} \in \Real^{6 \times 6} \geq 0 $. We also overload the logarithm function, $\MatLog{\cdot}$ to represent both $\LieGroupSE{3}$ and $\LieGroupSO{3}$ logarithmic maps as necessary. To account for gauge freedom, we fixed the first transformation to identity, $\Transform_{1,w} = \IdentityMatrix$, and initialized $\Transform_{2,w}$ to  $\Estimate{\Transform}_{2,1}$.  After convergence, we composed the final frame-to-frame estimate as $ \Transform_{2,1}^* =  \Transform_{2,w}^*  \left(\Transform_{1,w}^*\right)^{-1} = \Transform_{2,w}^*$.
%



\section{Robust Estimation}
Since \cref{eq:vo_linleastsquares} assigns cost values that grow quadratically with measurement error, it is very sensitive to outlier measurements.
A common solution to this problem is to replace the $L_2$ cost function with one that is less sensitive to large measurement errors \citep{MacTavish2015-wt}.
These robust cost functions are collectively known as M-estimators, and many variants exist. Each uses a re-weighting function, $\rho(\cdot)$,

\begin{equation}
  \Transform^* = \ArgMin{\Transform\in\text{SE}(3)}\sum_{i=1}^{N_t} 
  \rho\left(\Transpose{\Vector{e}_{i,t}} \ImageCovariance_{i,t}^{-1} \Vector{e}_{i,t}\right) = \ArgMin{\Transform\in\text{SE}(3)}\sum_{i=1}^{N_t} 
  \rho(\epsilon_i),
\end{equation}

where. given a parameter $c$, some common examples include:

\begin{align}
\rho(\epsilon) &= \left\{  	\begin{array}{ll}
		 \frac{c^2}{2} \log{\left(1 + \frac{\epsilon^2}{c^2}\right)}   & \mbox{Cauchy,} \\
		 \frac{1}{2} \frac{\epsilon^2}{c^2 + \epsilon^2}  & \mbox{Geman-McClure \citep{geman1992nonlinear},} \\
		 \\
		\left\{  	\begin{array}{ll}  \frac{\epsilon^2}{2} & \mbox{if} \Norm{\epsilon} < c \\
										c\Norm{\epsilon} - \frac{c^2}{2} & \mbox{if} \Norm{\epsilon} \geq c \end{array}
																						 \right.   & \mbox{Huber \citep{huber1964robust}.} \\
	\end{array}
	\right.
\end{align}




\section{Outstanding Issues}
There are several outstanding limitations of classical visual odometry pipelines that we can address with learned pseudo-sensors. 

\begin{table}[h!]
	\caption{\textbf{Data efficiency vs. computational efficiency}}	\begin{threeparttable}
	\begin{tabular}{m{0.68\textwidth}m{0.28\textwidth}}
		\toprule
		\textbf{Synopsis} & \textbf{Addressed by} \\ \midrule  
		Classical VO pipelines face a difficult-to-optimize trade-off between using all of the information contained within image and while still remaining computationally tractable.  & PROBE, DPC-Net, Sun-BCNN, HydraNet \\
		& \\
		\bottomrule
	\end{tabular}
\end{threeparttable}
\end{table}


\begin{table}[h!]
	\caption{\textbf{Systematic bias}}
	\begin{threeparttable}
	\begin{tabular}{m{0.68\textwidth}m{0.28\textwidth}}
		\toprule
		\textbf{Synopsis} & \textbf{Addressed by} \\ \midrule  
		Stereo visual odometry can incur systematic bias through poor extrinsic or intrinsic calibration, stereo triangulation errors, poor feature \textit{spread} (i.e., concentration of features on one side of an image), and poor data association due self-similar textures. &  DPC-Net \\
		& \\
		\bottomrule
	\end{tabular}
\end{threeparttable}
\end{table}


\begin{table}[h!]
	\caption{\textbf{Homoscedastic uncertainty}}
	\begin{threeparttable}
	\begin{tabular}{m{0.68\textwidth}m{0.28\textwidth}}
		\toprule
		\textbf{Synopsis} & \textbf{Addressed by} \\ \midrule  
		Stationary, homoscedastic noise in observation models can often reduce the consistency and accuracy of state estimates. This is especially true for complex, inferred measurement models. In visual data, inferred visual observations can be degraded not only due to sensor imperfections (e.g. poor intrinsic calibration, digitization effects, motion blur), but also as a result of the observed environment (e.g. self-similar scenes, specular surfaces, textureless environments). &  PROBE, Sun-BCNN, HydraNet \\
		& \\
		\bottomrule
	\end{tabular}
\end{threeparttable}
\end{table}

%
%
%\textbf{Adaptation to new environments}
%
%\begin{table}[h!]
%	\begin{threeparttable}
%	\begin{tabular}{m{0.68\textwidth}m{0.28\textwidth}}
%		\toprule
%		\textbf{Synopsis} & \textbf{Addressed by} \\ \midrule  
%		Traditional VO pipelines do little to adapt to new environments.
%. &  PROBE-GK, DPC-Net, HydraNet \\
%		& \\
%		\bottomrule
%	\end{tabular}
%\end{threeparttable}
%\end{table}

