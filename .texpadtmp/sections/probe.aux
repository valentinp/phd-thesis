\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Predictive Robust Estimation}{30}{chapter.4}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:probe}{{4}{30}{Predictive Robust Estimation}{chapter.4}{}}
\newlabel{ch:probe@cref}{{[chapter][4][]4}{[1][30][]30}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{30}{section.4.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces PROBE builds a predictive noise model for stereo visual odometry.\relax }}{30}{figure.caption.35}\protected@file@percent }
\newlabel{fig:probe_intro_fig}{{4.1}{30}{PROBE builds a predictive noise model for stereo visual odometry.\relax }{figure.caption.35}{}}
\newlabel{fig:probe_intro_fig@cref}{{[figure][1][4]4.1}{[1][30][]30}}
\citation{2015_Peretroukhin_PROBE}
\citation{2015_Peretroukhin_Get}
\citation{Peretroukhin2016-om}
\citation{censi2007accurate}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Motivation}{31}{section.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Related Work}{31}{section.4.3}\protected@file@percent }
\citation{gustafsson2000adaptive}
\citation{Hu2015-uw}
\citation{liu2018deep}
\citation{VegaBrown:ew,VegaBrown:2013fv}
\citation{ko2009gp}
\citation{wilson2011generalised}
\citation{melkumyan2011multi}
\citation{VegaBrown:ew}
\citation{VegaBrown:2013fv}
\citation{kerl2013robust}
\citation{Tsotsos2015}
\citation{Zhang2015}
\citation{ranftl2018deep}
\citation{VegaBrown:2013fv,VegaBrown:ew}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces PROBE builds a predictive noise model using Generalized Kernels (GK) for stereo visual odometry. The model can be trained with and without groundtruth egomotion.\relax }}{33}{figure.caption.36}\protected@file@percent }
\newlabel{fig:probe_gk_system}{{4.2}{33}{PROBE builds a predictive noise model using Generalized Kernels (GK) for stereo visual odometry. The model can be trained with and without groundtruth egomotion.\relax }{figure.caption.36}{}}
\newlabel{fig:probe_gk_system@cref}{{[figure][2][4]4.2}{[1][32][]33}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Predictive Robust Estimation for VO}{33}{section.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Bayesian Noise Model for Visual Odometry}{33}{subsection.4.4.1}\protected@file@percent }
\citation{Vega-Brown2014-sb}
\newlabel{eq:probe_vo_objective}{{4.2}{34}{Bayesian Noise Model for Visual Odometry}{equation.4.4.2}{}}
\newlabel{eq:probe_vo_objective@cref}{{[equation][2][4]4.2}{[1][33][]34}}
\newlabel{eq:probe_vo_objective}{{4.6}{34}{Bayesian Noise Model for Visual Odometry}{equation.4.4.6}{}}
\newlabel{eq:probe_vo_objective@cref}{{[equation][6][4]4.6}{[1][34][]34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Generalized Kernels}{34}{subsection.4.4.2}\protected@file@percent }
\citation{Vega-Brown2014-sb}
\citation{fitzgibbon2007learning}
\newlabel{eq:probe_posterior_bayes_rule}{{4.9}{35}{Generalized Kernels}{equation.4.4.9}{}}
\newlabel{eq:probe_posterior_bayes_rule@cref}{{[equation][9][4]4.9}{[1][35][]35}}
\newlabel{eq:probe_extended_likelihood}{{4.11}{35}{Generalized Kernels}{equation.4.4.11}{}}
\newlabel{eq:probe_extended_likelihood@cref}{{[equation][11][4]4.11}{[1][35][]35}}
\newlabel{eq:probe_kernel_likelihood}{{4.12}{35}{Generalized Kernels}{equation.4.4.12}{}}
\newlabel{eq:probe_kernel_likelihood@cref}{{[equation][12][4]4.12}{[1][35][]35}}
\newlabel{eq:probe_posterior}{{4.13}{35}{Generalized Kernels}{equation.4.4.13}{}}
\newlabel{eq:probe_posterior@cref}{{[equation][13][4]4.13}{[1][35][]35}}
\citation{Vega-Brown2014-sb}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}Generalized Kernels for Visual Odometry}{36}{subsection.4.4.3}\protected@file@percent }
\newlabel{eq:compute-psi}{{4.19}{36}{Generalized Kernels for Visual Odometry}{equation.4.4.19}{}}
\newlabel{eq:compute-psi@cref}{{[equation][19][4]4.19}{[1][36][]36}}
\newlabel{eq:compute-nu}{{4.20}{36}{Generalized Kernels for Visual Odometry}{equation.4.4.20}{}}
\newlabel{eq:compute-nu@cref}{{[equation][20][4]4.20}{[1][36][]36}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces Build the covariance model given a sequence of observations, $\mathcal  {D}$.\relax }}{37}{algorithm.1}\protected@file@percent }
\newlabel{alg:train-ground-truth}{{1}{37}{Build the covariance model given a sequence of observations, $\mathcal {D}$.\relax }{algorithm.1}{}}
\newlabel{alg:train-ground-truth@cref}{{[algorithm][1][]1}{[1][37][]37}}
\newlabel{eq:robust-loss}{{4.25}{37}{Generalized Kernels for Visual Odometry}{equation.4.4.25}{}}
\newlabel{eq:robust-loss@cref}{{[equation][25][4]4.25}{[1][37][]37}}
\citation{VegaBrown:2013fv}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Compute the transform between two images, given a set, $\mathcal  {I}_t$, of landmarks and predictors extracted from an image pair and a covariance model $\mathcal  {M}$. \relax }}{38}{algorithm.2}\protected@file@percent }
\newlabel{alg:compute-transform}{{2}{38}{Compute the transform between two images, given a set, $\mathcal {I}_t$, of landmarks and predictors extracted from an image pair and a covariance model $\mathcal {M}$. \relax }{algorithm.2}{}}
\newlabel{alg:compute-transform@cref}{{[algorithm][2][]2}{[1][38][]38}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.4}Inference without ground truth}{38}{subsection.4.4.4}\protected@file@percent }
\citation{dempster1977maximum}
\newlabel{eq:Qargmin}{{4.30}{39}{Inference without ground truth}{equation.4.4.30}{}}
\newlabel{eq:Qargmin@cref}{{[equation][30][4]4.30}{[1][39][]39}}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Prediction Space}{39}{section.4.5}\protected@file@percent }
\newlabel{sec:predictors}{{4.5}{39}{Prediction Space}{section.4.5}{}}
\newlabel{sec:predictors@cref}{{[section][5][4]4.5}{[1][39][]39}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Build the covariance model without ground truth given a sequence of observations, $\mathcal  {D'}$, and an initial odometry estimate $  \boldsymbol  {\mathbf  {T}}  _{1:T}^{(0)}$.\relax }}{40}{algorithm.3}\protected@file@percent }
\newlabel{alg:train-em}{{3}{40}{Build the covariance model without ground truth given a sequence of observations, $\mathcal {D'}$, and an initial odometry estimate $\Transform _{1:T}^{(0)}$.\relax }{algorithm.3}{}}
\newlabel{alg:train-em@cref}{{[algorithm][3][]3}{[1][39][]40}}
\citation{crete2007blur}
\citation{Furgale2013-sl}
\citation{crete2007blur}
\citation{crete2007blur}
\citation{crete2007blur}
\citation{crete2007blur}
\citation{crete2007blur}
\citation{Lucas:1981}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.1}Angular velocity and linear acceleration}{41}{subsection.4.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.2}Local image entropy}{41}{subsection.4.5.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.3}Blur}{41}{subsection.4.5.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces The Skybotix VI-Sensor, Point Grey Flea3, and checkerboard target used in our motion blur experiments.\relax }}{42}{figure.caption.37}\protected@file@percent }
\newlabel{fig:probe_tricifix}{{4.3}{42}{The Skybotix VI-Sensor, Point Grey Flea3, and checkerboard target used in our motion blur experiments.\relax }{figure.caption.37}{}}
\newlabel{fig:probe_tricifix@cref}{{[figure][3][4]4.3}{[1][41][]42}}
\newlabel{fig:probe_visensor_reprojectionError}{{\caption@xref {fig:probe_visensor_reprojectionError}{ on input line 499}}{42}{Blur}{figure.caption.38}{}}
\newlabel{fig:probe_visensor_reprojectionError@cref}{{[subsection][3][4,5]4.5.3}{[1][41][]42}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Reprojection error of checkerboard corners triangulated from the VI-Sensor and reprojected into the Flea3.We distinguish between high and low blur by thresholding the blur metric \cite  {crete2007blur}.\relax }}{42}{figure.caption.38}\protected@file@percent }
\newlabel{fig:probe_visensor_trackingError}{{\caption@xref {fig:probe_visensor_trackingError}{ on input line 506}}{42}{Blur}{figure.caption.39}{}}
\newlabel{fig:probe_visensor_trackingError@cref}{{[subsection][3][4,5]4.5.3}{[1][41][]42}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Effect of blur on reprojection and tracking error for the slow-then-fast checkerboard dataset. We distinguish between high and low blur by thresholding the blur metric \cite  {crete2007blur}. The variance in both errors increases with blur.\relax }}{42}{figure.caption.39}\protected@file@percent }
\newlabel{fig:probe_visensor_histograms}{{4.5}{42}{Effect of blur on reprojection and tracking error for the slow-then-fast checkerboard dataset. We distinguish between high and low blur by thresholding the blur metric \cite {crete2007blur}. The variance in both errors increases with blur.\relax }{figure.caption.39}{}}
\newlabel{fig:probe_visensor_histograms@cref}{{[figure][5][4]4.5}{[1][41][]42}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.4}Optical flow variance}{43}{subsection.4.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5.5}Image frequency composition}{43}{subsection.4.5.5}\protected@file@percent }
\citation{Barfoot2014-ac}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces The optical flow variance predictor can help in detecting moving objects. Red circles correspond to higher values of the optical flow variance score (i.e., features more likely to belong to a moving object).\relax }}{44}{figure.caption.40}\protected@file@percent }
\newlabel{fig:probe_flow_variance}{{4.6}{44}{The optical flow variance predictor can help in detecting moving objects. Red circles correspond to higher values of the optical flow variance score (i.e., features more likely to belong to a moving object).\relax }{figure.caption.40}{}}
\newlabel{fig:probe_flow_variance@cref}{{[figure][6][4]4.6}{[1][43][]44}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces A high-frequency predictor can distinguish between regions of high and low texture such as foliage and shadows. Green indicates higher values.\relax }}{44}{figure.caption.41}\protected@file@percent }
\newlabel{fig:probe_high_frequency}{{4.7}{44}{A high-frequency predictor can distinguish between regions of high and low texture such as foliage and shadows. Green indicates higher values.\relax }{figure.caption.41}{}}
\newlabel{fig:probe_high_frequency@cref}{{[figure][7][4]4.7}{[1][43][]44}}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}Experiments}{44}{section.4.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.1}Simulation}{44}{subsection.4.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Monte-Carlo Verification}{44}{section*.42}\protected@file@percent }
\citation{kerl2013robust}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces Mean Frobenius norm of the error between the estimated and true noise covariance as a function of training data size. The norm tends to zero as training data is added which indicates that PROBE-GK is learning the correct covariances.\relax }}{45}{figure.caption.43}\protected@file@percent }
\newlabel{fig:probe_frobNorm}{{4.8}{45}{Mean Frobenius norm of the error between the estimated and true noise covariance as a function of training data size. The norm tends to zero as training data is added which indicates that PROBE-GK is learning the correct covariances.\relax }{figure.caption.43}{}}
\newlabel{fig:probe_frobNorm@cref}{{[figure][8][4]4.8}{[1][44][]45}}
\newlabel{fig:probe_SimWorld}{{\caption@xref {fig:probe_SimWorld}{ on input line 589}}{45}{Synthetic}{figure.caption.45}{}}
\newlabel{fig:probe_SimWorld@cref}{{[subsection][1][4,6]4.6.1}{[1][45][]45}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces Our synthetic world. A stereo camera rig moves through a world with 2000 point features.\relax }}{45}{figure.caption.45}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Synthetic}{45}{section*.44}\protected@file@percent }
\citation{Geiger2013-ky}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces A comparison of translational and rotational Root Mean Square Error on simulated data (RMSE) for four different stereo-visual odometry pipelines: two baseline bundle adjustment procedures with and without a robust Student's\nobreakspace  {}$t$ cost with a fixed and hand-tuned covariance and degrees of freedom (M-Estimation), a robust bundle adjustment with covariances learned from ground truth with \cref  {alg:train-ground-truth} (GK-GT), and a robust bundle adjustment using covariances learned without ground truth using expectation maximization, with \cref  {alg:train-em} (GK-EM). Note in this experiment, the RMSE curves for GK-GT and GK-EM very nearly overlap. The overall translational and rotational ARMSE values are shown in Table \ref  {table:probe-gk_armse_errors}.\relax }}{46}{figure.caption.46}\protected@file@percent }
\newlabel{fig:probe_sim_comparison}{{4.10}{46}{A comparison of translational and rotational Root Mean Square Error on simulated data (RMSE) for four different stereo-visual odometry pipelines: two baseline bundle adjustment procedures with and without a robust Student's~$t$ cost with a fixed and hand-tuned covariance and degrees of freedom (M-Estimation), a robust bundle adjustment with covariances learned from ground truth with \cref {alg:train-ground-truth} (GK-GT), and a robust bundle adjustment using covariances learned without ground truth using expectation maximization, with \cref {alg:train-em} (GK-EM). Note in this experiment, the RMSE curves for GK-GT and GK-EM very nearly overlap. The overall translational and rotational ARMSE values are shown in Table \ref {table:probe-gk_armse_errors}.\relax }{figure.caption.46}{}}
\newlabel{fig:probe_sim_comparison@cref}{{[figure][10][4]4.10}{[1][46][]46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.2}KITTI}{46}{subsection.4.6.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces The KITTI dataset contains three different environments. We validate PROBE-GK by training on each type and testing against a baseline stereo visual odometry pipeline.\relax }}{46}{figure.caption.47}\protected@file@percent }
\newlabel{fig:kitti_environments}{{4.11}{46}{The KITTI dataset contains three different environments. We validate PROBE-GK by training on each type and testing against a baseline stereo visual odometry pipeline.\relax }{figure.caption.47}{}}
\newlabel{fig:kitti_environments@cref}{{[figure][11][4]4.11}{[1][46][]46}}
\newlabel{fig:probe-gk_kitti_comparison1}{{4.12a}{47}{City category.\relax }{figure.caption.48}{}}
\newlabel{fig:probe-gk_kitti_comparison1@cref}{{[subfigure][1][4,12]4.12a}{[1][47][]47}}
\newlabel{sub@fig:probe-gk_kitti_comparison1}{{a}{47}{City category.\relax }{figure.caption.48}{}}
\newlabel{sub@fig:probe-gk_kitti_comparison1@cref}{{[subfigure][1][4,12]4.12a}{[1][47][]47}}
\newlabel{fig:probe-gk_kitti_comparison2}{{4.12b}{47}{Residential category.\relax }{figure.caption.48}{}}
\newlabel{fig:probe-gk_kitti_comparison2@cref}{{[subfigure][2][4,12]4.12b}{[1][47][]47}}
\newlabel{sub@fig:probe-gk_kitti_comparison2}{{b}{47}{Residential category.\relax }{figure.caption.48}{}}
\newlabel{sub@fig:probe-gk_kitti_comparison2@cref}{{[subfigure][2][4,12]4.12b}{[1][47][]47}}
\newlabel{fig:probe-gk_kitti_comparison3}{{4.12c}{47}{Road category.\relax }{figure.caption.48}{}}
\newlabel{fig:probe-gk_kitti_comparison3@cref}{{[subfigure][3][4,12]4.12c}{[1][47][]47}}
\newlabel{sub@fig:probe-gk_kitti_comparison3}{{c}{47}{Road category.\relax }{figure.caption.48}{}}
\newlabel{sub@fig:probe-gk_kitti_comparison3@cref}{{[subfigure][3][4,12]4.12c}{[1][47][]47}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces RMSE comparison of stereo VO estimators evaluated on data in the KITTI odometry benchmark. See \Cref  {table:probe-gk_armse_errors} for a quantitative summary.\relax }}{47}{figure.caption.48}\protected@file@percent }
\newlabel{fig:probe-gk_kitti_comparison_overall}{{4.12}{47}{RMSE comparison of stereo VO estimators evaluated on data in the KITTI odometry benchmark. See \Cref {table:probe-gk_armse_errors} for a quantitative summary.\relax }{figure.caption.48}{}}
\newlabel{fig:probe-gk_kitti_comparison_overall@cref}{{[figure][12][4]4.12}{[1][47][]47}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Comparison of average root mean squared errors (ARMSE) for rotational and translational components. Each trial is trained and tested from a particular category of raw data from the synthetic and KITTI datasets.\relax }}{48}{table.caption.49}\protected@file@percent }
\newlabel{table:probe-gk_armse_errors}{{4.1}{48}{Comparison of average root mean squared errors (ARMSE) for rotational and translational components. Each trial is trained and tested from a particular category of raw data from the synthetic and KITTI datasets.\relax }{table.caption.49}{}}
\newlabel{table:probe-gk_armse_errors@cref}{{[table][1][4]4.1}{[1][48][]48}}
\newlabel{fig:probe-gk_experiments}{{4.13a}{48}{Our experimental apparatus: a Clearpath Husky rover outfitted with a PointGrey XB3 stereo camera and a differential GPS receiver and base station.\relax }{figure.caption.50}{}}
\newlabel{fig:probe-gk_experiments@cref}{{[subfigure][1][4,13]4.13a}{[1][48][]48}}
\newlabel{sub@fig:probe-gk_experiments}{{a}{48}{Our experimental apparatus: a Clearpath Husky rover outfitted with a PointGrey XB3 stereo camera and a differential GPS receiver and base station.\relax }{figure.caption.50}{}}
\newlabel{sub@fig:probe-gk_experiments@cref}{{[subfigure][1][4,13]4.13a}{[1][48][]48}}
\newlabel{fig:probe-gk_experiment_groundtruth}{{4.13b}{48}{GPS ground truth for 5 experimental trials collected near the UTIAS Mars Dome. Each trial is approximately 250 m long.\relax }{figure.caption.50}{}}
\newlabel{fig:probe-gk_experiment_groundtruth@cref}{{[subfigure][2][4,13]4.13b}{[1][48][]48}}
\newlabel{sub@fig:probe-gk_experiment_groundtruth}{{b}{48}{GPS ground truth for 5 experimental trials collected near the UTIAS Mars Dome. Each trial is approximately 250 m long.\relax }{figure.caption.50}{}}
\newlabel{sub@fig:probe-gk_experiment_groundtruth@cref}{{[subfigure][2][4,13]4.13b}{[1][48][]48}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Our experimental setup.\relax }}{48}{figure.caption.50}\protected@file@percent }
\newlabel{fig:probe-gk_experiments_overall}{{4.13}{48}{Our experimental setup.\relax }{figure.caption.50}{}}
\newlabel{fig:probe-gk_experiments_overall@cref}{{[figure][13][4]4.13}{[1][48][]48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6.3}UTIAS}{48}{subsection.4.6.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces Training without ground truth using PROBE-GK-EM on a 250.2m path around the Mars Dome at UTIAS. The likelihood of the data increases with each iteration, and the loop closure error decreases, improving significantly from a baseline static M-estimator.\relax }}{49}{figure.caption.51}\protected@file@percent }
\newlabel{fig:probe-gk_experiments_trainingstats}{{4.14}{49}{Training without ground truth using PROBE-GK-EM on a 250.2m path around the Mars Dome at UTIAS. The likelihood of the data increases with each iteration, and the loop closure error decreases, improving significantly from a baseline static M-estimator.\relax }{figure.caption.51}{}}
\newlabel{fig:probe-gk_experiments_trainingstats@cref}{{[figure][14][4]4.14}{[1][48][]49}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Comparison of loop closure errors for 4 different experimental trials with and without a learned PROBE-GK-EM model.\relax }}{49}{table.caption.52}\protected@file@percent }
\newlabel{table:probe-gk_loop_closure_errors}{{4.2}{49}{Comparison of loop closure errors for 4 different experimental trials with and without a learned PROBE-GK-EM model.\relax }{table.caption.52}{}}
\newlabel{table:probe-gk_loop_closure_errors@cref}{{[table][2][4]4.2}{[1][49][]49}}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Summary}{49}{section.4.7}\protected@file@percent }
\@setckpt{sections/probe}{
\setcounter{page}{51}
\setcounter{equation}{32}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{3}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{7}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{14}
\setcounter{table}{2}
\setcounter{caption@flags}{2}
\setcounter{ContinuedFloat}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{11}
\setcounter{Hfootnote}{13}
\setcounter{bookmark@seq@number}{23}
\setcounter{ALG@line}{22}
\setcounter{ALG@rem}{22}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{float@type}{16}
\setcounter{algorithm}{3}
\setcounter{NAT@ctr}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{lstnumber}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{section@level}{1}
\setcounter{lstlisting}{0}
}
